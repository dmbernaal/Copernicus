
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: dev_nb/copernicusInit_001.ipynb

import torch.nn as nn
import torch.nn.functional as F
import torch
from functools import partial
import math

# selu init
# custom initialization method when using SeLU Activations
def selu_normal_(tensor, mode1='fan_in', mode2='fan_out'):
    fan_in = nn.init._calculate_correct_fan(tensor, mode1)
    fan_out = nn.init._calculate_correct_fan(tensor, mode2)
    with torch.no_grad():
        return torch.randn(fan_in, fan_out) / math.sqrt(1./fan_in)

nn.init.selu_normal_ = selu_normal_ # adding to nn package

# # init method Deprecated
# def init_nn(m, init_method, act=None):
#     """
#     Main function to initialize the nn
#     m: model
#     init_method: <nn.init method>
#         example: nn.init.selu_normal_
#     """
#     if getattr(m, 'bias', None) is not None: nn.init.constant_(m.bias, 0) # for batchnorm layers
#     if isinstance(m, (nn.Linear)):
#         # init based on act
#         if act is None or act == 'relu':
#             gain = math.sqrt(2)
#             try: init_method(m.weight, gain=gain)
#             except: init_method(m.weight)
#         elif act == 'leakyrelu':
#             gain = nn.calculate_gain('leaky_relu', 0.01)
#             try: init_method(m.weight, gain=gain)
#             except: init_method(m.weight)
#         else: init_method(m.weight) # init weights with init_method
# #     for l in m.children(): init_nn(l, init_method, act) # recursion

# init method
def init_nn(model, init_method):
    """
    Main function to initialize the nn
    m: model
    init_method: <nn.init method>
        example: nn.init.selu_normal_
    """
    for l in model.children():
        if isinstance(l, (nn.Sequential)):
            for layer in l[:-1]:
                for li in layer:
                    if isinstance(li, nn.Linear):
                        if getattr(li, 'bias', None) is not None: nn.init.constant_(li.bias, 0)
                        init_method(li.weight)
            # init head
            init_method(l[-1].weight)
            if getattr(l[-1], 'bias', None) is not None: nn.init.constant_(l[-1].bias, 0)
        
        
def get_init_(init=None):
    if init is None: init_method = nn.init.kaiming_normal_
    elif init == 'uniform': init_method = nn.init.uniform_
    elif init == 'normal': init_method = nn.init.normal_
    elif init == 'ones': init_method = nn.init.ones_
    elif init == 'zeros': init_method = nn.init.zeros_
    elif init == 'eye': init_method = nn.init.eye_
    elif init == 'xavier_uniform': init_method = nn.init.xavier_uniform_
    elif init == 'xavier_normal': init_method = nn.init.xavier_normal_
    elif init == 'kaiming_uniform': init_method = nn.init.kaiming_uniform_
    elif init == 'kaiming_normal': init_method = nn.init.kaiming_normal_
    elif init == 'orthogonal': init_method = nn.init.orthogonal_
    else: raise ValueError('wrong init: Check initlist')
    
    return init_method

initlist = ['uniform', 'normal', 'ones', 'xavier_uniform', 'xavier_normal', 'kaiming_uniform', 'kaiming_normal', 'orthogonal']