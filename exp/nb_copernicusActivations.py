
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: dev_nb/copernicusActivations_001.ipynb

import torch
import math
import torch.nn.functional as F
import torch.nn as nn

# mish activation
class Mish(nn.Module):
    def __init__(self): super().__init__()
    def forward(self, x): return x * ( torch.tanh(F.softplus(x)))

class Selu(nn.Module):
    def __init__(self):
        super().__init__()
        self.alpha = torch.tensor(1.6732632423543772848170429916717)
        self.scale = torch.tensor(1.0507009873554804934193349852946)
 
    def forward(self, x):
        return self.scale * torch.where(x>=0.0, x, self.alpha * torch.exp(x) - self.alpha)
    
def get_activation_(act):
    if act is None or act == 'relu': act_fn = nn.ReLU(inplace=True) # relu as default
    elif act == 'mish': act_fn = Mish()
    elif act == 'selu': act_fn = Selu()
    elif act == 'elu': act_fn = nn.ELU()
    elif act == 'hardshrink': act_fn = nn.Hardshrink()
    elif act == 'hardtanh': act_fn = nn.Hardtanh()
    elif act == 'leakyrelu': act_fn = nn.LeakyReLU()
    elif act == 'logsigmoid': act_fn = nn.LogSigmoid()
    elif act == 'prelu': act_fn = nn.PReLU()
    elif act == 'relu6': act_fn = nn.ReLU6()
    elif act == 'rrelu': act_fn = nn.RReLU()
    elif act == 'celu': act_fn = nn.CELU()
    elif act == 'sigmoid': act_fn = nn.Sigmoid()
    elif act == 'softplus': act_fn = nn.Softplus()
    elif act == 'softshrink': act_fn = nn.Softshrink()
    elif act == 'softsign': act_fn = nn.Softsign()
    elif act == 'tanhshrink': act_fn = nn.Tanhshrink()
    else:
        raise ValueError('Act is not properly defined: check activations list')
    
    return act_fn

activations = ['mish', 'selu', 'elu', 'relu', 'hardshrink', 'hardtanh', 'leakyrelu', 'logsigmoid', 'prelu', 'rrelu', 'relu6', 'celu', 'sigmoid', 'softplus', 'softshrink', 'softsign', 'tanhshrink']