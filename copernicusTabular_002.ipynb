{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Copernicus Tabular\n",
    "### ```autotabular``` \n",
    "This notebook follows ```001``` with the addition of ```AutoTabularBunch``` which will allow for the creation of a ```TabularBunch``` without declaring cont and cat vars but only dependent var.\n",
    "\n",
    "We will be experimenting with a dataset that contains both ```cont``` and ```cat``` variables.\n",
    "\n",
    "## VERSION Alpha: 0.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# dependencies & imports\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from path import Path\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.set_device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# root directory to all data\n",
    "root = Path('./data')\n",
    "(root/'ibm_attrition').listdir()\n",
    "f = os.listdir(root/'ibm_attrition')[0]\n",
    "data = pd.read_csv(f'{root}/ibm_attrition/{f}', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Attrition</th>\n",
       "      <th>BusinessTravel</th>\n",
       "      <th>DailyRate</th>\n",
       "      <th>Department</th>\n",
       "      <th>DistanceFromHome</th>\n",
       "      <th>Education</th>\n",
       "      <th>EducationField</th>\n",
       "      <th>EmployeeCount</th>\n",
       "      <th>EmployeeNumber</th>\n",
       "      <th>...</th>\n",
       "      <th>RelationshipSatisfaction</th>\n",
       "      <th>StandardHours</th>\n",
       "      <th>StockOptionLevel</th>\n",
       "      <th>TotalWorkingYears</th>\n",
       "      <th>TrainingTimesLastYear</th>\n",
       "      <th>WorkLifeBalance</th>\n",
       "      <th>YearsAtCompany</th>\n",
       "      <th>YearsInCurrentRole</th>\n",
       "      <th>YearsSinceLastPromotion</th>\n",
       "      <th>YearsWithCurrManager</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>41</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Travel_Rarely</td>\n",
       "      <td>1102</td>\n",
       "      <td>Sales</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Life Sciences</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>49</td>\n",
       "      <td>No</td>\n",
       "      <td>Travel_Frequently</td>\n",
       "      <td>279</td>\n",
       "      <td>Research &amp; Development</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>Life Sciences</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Travel_Rarely</td>\n",
       "      <td>1373</td>\n",
       "      <td>Research &amp; Development</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Other</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33</td>\n",
       "      <td>No</td>\n",
       "      <td>Travel_Frequently</td>\n",
       "      <td>1392</td>\n",
       "      <td>Research &amp; Development</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>Life Sciences</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>27</td>\n",
       "      <td>No</td>\n",
       "      <td>Travel_Rarely</td>\n",
       "      <td>591</td>\n",
       "      <td>Research &amp; Development</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Medical</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Age Attrition     BusinessTravel  DailyRate              Department  \\\n",
       "0   41       Yes      Travel_Rarely       1102                   Sales   \n",
       "1   49        No  Travel_Frequently        279  Research & Development   \n",
       "2   37       Yes      Travel_Rarely       1373  Research & Development   \n",
       "3   33        No  Travel_Frequently       1392  Research & Development   \n",
       "4   27        No      Travel_Rarely        591  Research & Development   \n",
       "\n",
       "   DistanceFromHome  Education EducationField  EmployeeCount  EmployeeNumber  \\\n",
       "0                 1          2  Life Sciences              1               1   \n",
       "1                 8          1  Life Sciences              1               2   \n",
       "2                 2          2          Other              1               4   \n",
       "3                 3          4  Life Sciences              1               5   \n",
       "4                 2          1        Medical              1               7   \n",
       "\n",
       "   ...  RelationshipSatisfaction StandardHours  StockOptionLevel  \\\n",
       "0  ...                         1            80                 0   \n",
       "1  ...                         4            80                 1   \n",
       "2  ...                         2            80                 0   \n",
       "3  ...                         3            80                 0   \n",
       "4  ...                         4            80                 1   \n",
       "\n",
       "   TotalWorkingYears  TrainingTimesLastYear WorkLifeBalance  YearsAtCompany  \\\n",
       "0                  8                      0               1               6   \n",
       "1                 10                      3               3              10   \n",
       "2                  7                      3               3               0   \n",
       "3                  8                      3               3               8   \n",
       "4                  6                      3               3               2   \n",
       "\n",
       "  YearsInCurrentRole  YearsSinceLastPromotion  YearsWithCurrManager  \n",
       "0                  4                        0                     5  \n",
       "1                  7                        1                     7  \n",
       "2                  0                        0                     0  \n",
       "3                  7                        3                     0  \n",
       "4                  2                        2                     2  \n",
       "\n",
       "[5 rows x 35 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ```TabularDataset``` 0.0.1\n",
    "In this attempt we will just create a simple datatset class that takes in raw data and will split between: ```X Y``` and create seperate ```x_train, x_val, y_train y_val``` which will individually containm ```X1, X2, Y```.\n",
    "\n",
    "This class will also perform all pre-processing from removing and replacing NAs and perform LabelEncoding to both Categorical variables and our Dependent Variables: A class-map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# 1.\n",
    "class TabularBunch():\n",
    "    \"\"\"\n",
    "    \n",
    "    NOTE: Deprecated: Need to fix multi-layered categorical filterring\n",
    "    \n",
    "    df: <pandas df>\n",
    "    categorical: <df[col]>: columns which represent all categorical (discrete) variables\n",
    "    \n",
    "    This class will turn all columns but categorical & dependent into continuous variables. IF there aren't any categorical (categorical==None) then we will treat all but dependent variables as continuous.\n",
    "    \n",
    "    NB: Perform some data analysis and exploration before calling this method. For later versions we will automate this full process.\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, df, categorical=None, dependent=None, null_thresh=0., verbose=True):\n",
    "        super().__init__()\n",
    "        assert dependent # needs dependent variable to work\n",
    "        self.df = df.copy()\n",
    "        self.categorical = categorical\n",
    "        self.verbose = verbose\n",
    "        \n",
    "        # cleaning df <OPTIONAL>: removing null values from certain threshold. \n",
    "        if null_thresh > 0.:\n",
    "            # this will clear our categorical list: as we have to clean some columns\n",
    "            self.categorical = self.purge_nulls_(null_thresh)\n",
    "            \n",
    "        # splitting X, y: Initial split | no cat/cont split\n",
    "        self.Y = self.df[dependent] # all rows should have a target | dependent variable\n",
    "        X = self.df.drop(columns=dependent, axis=1)\n",
    "            \n",
    "        # creating classes\n",
    "        self.classes = set(y)\n",
    "        ### TO DO:\n",
    "        ### Create class index to class string mapping\n",
    "        ### This will help with any confusion and inference level interpretation\n",
    "\n",
    "        \n",
    "        # filling rest of null values: will perform just incase\n",
    "        X = self.fill_na_(X)\n",
    "            \n",
    "        # Splitting X into: x_cont, x_cat\n",
    "        # x_cont: continuous variables\n",
    "        # x_cat: categorical (discrete variables which will go into embeddings)\n",
    "        if categorical is not None: \n",
    "            self.X_cat = X[self.categorical].copy()\n",
    "            self.X_cont = X.drop(columns=self.categorical, axis=1).copy()\n",
    "            \n",
    "        else:\n",
    "            self.X_cat = None\n",
    "            self.X_cont = X.copy() # do nothing: we will use this placeholder for now\n",
    "            \n",
    "        # Turning our categorical dataset: X_cat into LabelEncodings\n",
    "        # This will turn cat1 -> 0, cat2 -> 1, for each cat within\n",
    "        # The category columns\n",
    "        self.labelencoding_()\n",
    "                \n",
    "        # Creating our embedding dictionaries\n",
    "        # this will help with forming nn.Embeddings\n",
    "        # and will also help with creating our datasets\n",
    "        if categorical:\n",
    "            self.emb_c = {n: len(col.cat.categories) for n,col in self.X_cat.items()}\n",
    "            self.emb_sz = [(c, min(50, (c+1)//2)) for _,c  in self.emb_c.items()]\n",
    "            self.emb_cols = self.categorical\n",
    "        \n",
    "        # our X, Y \n",
    "        # this will be used to feed into our dataset class\n",
    "        if self.X_cat is not None:\n",
    "            if len(self.X_cont.columns) == 0:\n",
    "                self.X = self.X_cat\n",
    "            else:\n",
    "                self.X = pd.concat([self.X_cat, self.X_cont], axis=1)\n",
    "        else:\n",
    "            self.X = self.X_cont\n",
    "            \n",
    "        if self.X_cat is not None and self.X_cont is not None:\n",
    "            self.X_state = 'both'\n",
    "        elif self.X_cat is not None and self.X_cont is None:\n",
    "            self.X_state = 'catonly'\n",
    "        elif self.X_cat is None and self.X_cont is not None:\n",
    "            self.X_state = 'contonly'\n",
    "        \n",
    "        # Clearning our attributes: Don't need to store everything\n",
    "        del self.X_cat\n",
    "        del self.X_cont\n",
    "        del self.categorical\n",
    "        del self.verbose\n",
    "        del self.df\n",
    "        \n",
    "        if verbose: print('Finished!')\n",
    "            \n",
    "    def purge_nulls_(self, null_thresh):\n",
    "        \"\"\"Will remove all columns with null values exceeding our threshold\"\"\"\n",
    "        if self.verbose: print(f'Performing null purge. Null threshold: {null_thresh}...')\n",
    "        \n",
    "        nt = int(len(self.df) * null_thresh)\n",
    "        \n",
    "        for col in self.df.columns:\n",
    "            if self.df[col].isnull().sum() > nt:\n",
    "                self.df.drop(col, axis=1, inplace=True)\n",
    "                \n",
    "        cols = list(self.df.columns)\n",
    "        categorical = self.categorical_left_(cols) # removing categories that are missing\n",
    "        return categorical\n",
    "                \n",
    "    def fill_na_(self, X):\n",
    "        \"\"\"Will perform a quick processing of NA values.\"\"\"\n",
    "        if self.verbose: print(f'Performing NaN Replacement...')\n",
    "        for col in X.columns:\n",
    "            if X.dtypes[col] == \"object\":\n",
    "                X[col] = X[col].fillna(\"NA\")\n",
    "            else:\n",
    "                X[col] = X[col].fillna(0)\n",
    "        return X\n",
    "        \n",
    "    \n",
    "    def labelencoding_(self):\n",
    "        if self.verbose: print(f'Performing label encoding operation. This may take a few seconds...')\n",
    "        if self.X_cat is not None:\n",
    "            for col in self.X_cat.columns:\n",
    "                le = LabelEncoder()\n",
    "                self.X_cat[col] = le.fit_transform(self.X_cat[col])\n",
    "                self.X_cat[col] = self.X_cat[col].astype('category')\n",
    "                \n",
    "        # encoding our y\n",
    "        le = LabelEncoder()\n",
    "        self.Y = le.fit_transform(self.Y)\n",
    "                \n",
    "    def categories_left_(self, cols):\n",
    "        \"\"\"\n",
    "        This method will return a list of all categorical columns that are left after purging our high-threshold NAs. This will simply update our list therefor not return anything.\n",
    "        \"\"\"\n",
    "        categorical_left = pd.Series()\n",
    "        for col in self.categorical:\n",
    "            mask = cols == col\n",
    "            masks = mask|masks\n",
    "        return categories_left\n",
    "    \n",
    "    def get_train_test_(self, test_size=0.1):\n",
    "        \"\"\"\n",
    "        This get_train_test_ method will take in a bunch object and append x_train, x_val, y_train, y_val using train_test_split from the scikit learn libray. \n",
    "\n",
    "        This will allow for everything to be kept in a single bunch object which will feed into the Learner class and Dataset class\n",
    "        \"\"\"\n",
    "        self.x_train, self.x_val, self.y_train, self.y_val = train_test_split(self.X,self.Y, test_size=test_size, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# 1b.\n",
    "class AutoTabularBunch():\n",
    "    \"\"\"\n",
    "    \n",
    "    df: <pandas df>\n",
    "    categorical: <df[col]>: columns which represent all categorical (discrete) variables\n",
    "    purge_: <string | condition>: This will determine how to handle null values. smart: will fill na with approipriate values using a smart fill algorithm (coming soon)\n",
    "        To feed a data into our Learner object (tabularlearner | NN) we cannot feed NaNs\n",
    "    \n",
    "    This class will turn all columns but categorical & dependent into continuous variables. IF there aren't any categorical (categorical==None) then we will treat all but dependent variables as continuous.\n",
    "    \n",
    "    NB: Perform some data analysis and exploration before calling this method. For later versions we will automate this full process.\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, df, dependent=None, problem_type=None, null_thresh=.1, cat_thresh=0.9, purge_=None, verbose=True):\n",
    "        super().__init__()\n",
    "        # init declarations\n",
    "        self.problem_type = 'classification' if problem_type is None else problem_type\n",
    "        purge = 'null_thresh' if purge_ is None else purge_\n",
    "        assert dependent # needs dependent variable to work\n",
    "        assert self.problem_type == 'classification' or self.problem_type == 'regression'\n",
    "        assert purge == 'aggressive' or purge == 'null_thresh' or purge == \"smart\"\n",
    "        self.df = df.copy()\n",
    "        self.verbose = verbose\n",
    "        self.dependent = dependent\n",
    "        \n",
    "        # cleaning df <OPTIONAL>: removing null values from certain threshold.\n",
    "        if purge == 'null_thresh':\n",
    "            if null_thresh > 0.:\n",
    "                # this will clear our categorical list: as we have to clean some columns\n",
    "                self.purge_nulls_(null_thresh)\n",
    "                \n",
    "                if self.df[self.dependent].isnull().any():\n",
    "                    raise ValueError(f'{self.dependent} column seems to have some missing values or NaN values. This will cause an error when training the model. Suggestion: Use aggressive purge_ type or pre-process the data to have no-nan values on this specific column')\n",
    "        elif purge == 'aggressive':                        \n",
    "            if verbose:\n",
    "                print('Purging all rows with any null values: this is necessary for training otherwise use smart purge')\n",
    "            mask = self.df[dependent].isnull()\n",
    "            self.df = self.df[mask==False]\n",
    "        elif purge == \"smart\":\n",
    "            print('coming soon: This will fillna with appropriate values')\n",
    "            \n",
    "\n",
    "        # splitting X, y: Initial split | no cat/cont split\n",
    "        self.Y = self.df[dependent] # all rows should have a target | dependent variable\n",
    "        X = self.df.drop(columns=dependent, axis=1)\n",
    "        \n",
    "        # Grabbing categorical and setting as 'category' object time\n",
    "        self.categorical = self.set_categorical(cat_thresh, X)\n",
    "            \n",
    "        # creating classes\n",
    "        self.classes = set(self.Y)\n",
    "        self.nc = len(self.classes)\n",
    "        ### TO DO:\n",
    "        ### Create class index to class string mapping\n",
    "        ### This will help with any confusion and inference level interpretation\n",
    "\n",
    "        \n",
    "        # filling rest of null values: will perform just incase\n",
    "        X = self.fill_na_(X)\n",
    "            \n",
    "        # Splitting X into: x_cont, x_cat\n",
    "        # x_cont: continuous variables\n",
    "        # x_cat: categorical (discrete variables which will go into embeddings)\n",
    "        if self.categorical is not None: \n",
    "            self.X_cat = X[self.categorical].copy()\n",
    "            self.X_cont = X.drop(columns=self.categorical, axis=1).copy()\n",
    "            self.n_cont = len(self.X_cont.columns)\n",
    "            \n",
    "        else:\n",
    "            self.X_cat = pd.DataFrame([])\n",
    "            self.X_cont = X.copy() # do nothing: we will use this placeholder for now\n",
    "            self.n_cont = None\n",
    "            \n",
    "        # Turning our categorical dataset: X_cat into LabelEncodings\n",
    "        # This will turn cat1 -> 0, cat2 -> 1, for each cat within\n",
    "        # The category columns\n",
    "        self.labelencoding_()\n",
    "                \n",
    "        # Creating our embedding dictionaries\n",
    "        # this will help with forming nn.Embeddings\n",
    "        # and will also help with creating our datasets\n",
    "        if self.categorical is not None:\n",
    "            self.emb_c = {n: len(col.cat.categories) for n,col in self.X_cat.items()}\n",
    "            self.emb_sz = [(c, min(50, (c+1)//2)) for _,c  in self.emb_c.items()]\n",
    "            self.emb_cols = self.categorical\n",
    "        \n",
    "        # our X, Y \n",
    "        # this will be used to feed into our dataset class\n",
    "        if self.X_cat is not None:\n",
    "            if len(self.X_cont.columns) == 0:\n",
    "                self.X = self.X_cat\n",
    "            else:\n",
    "                self.X = pd.concat([self.X_cat, self.X_cont], axis=1)\n",
    "        else:\n",
    "            self.X = self.X_cont\n",
    "        \n",
    "#         if self.X_cat is not None and self.X_cont is not None:\n",
    "#             self.X_state = 'both'\n",
    "#         elif self.X_cat is not None and self.X_cont is None:\n",
    "#             self.X_state = 'catonly'\n",
    "#         elif self.X_cat is None and self.X_cont is not None:\n",
    "#             self.X_state = 'contonly'\n",
    "\n",
    "        if self.X_cat.size and self.X_cont.size:\n",
    "            self.X_state = 'both'\n",
    "        elif self.X_cat.size and not self.X_cont.size:\n",
    "            self.X_state = 'catonly'\n",
    "        elif not self.X_cat.size and self.X_cont.size:\n",
    "            self.X_state = 'contonly'\n",
    "        \n",
    "        # Clearning our attributes: Don't need to store everything\n",
    "        del self.X_cat\n",
    "        del self.X_cont\n",
    "        del self.categorical\n",
    "        del self.verbose\n",
    "        del self.df\n",
    "        \n",
    "        if verbose: print('Finished!')\n",
    "            \n",
    "    def purge_nulls_(self, null_thresh):\n",
    "        \"\"\"Will remove all columns with null values exceeding our threshold\"\"\"\n",
    "        if self.verbose: print(f'Performing null purge. Null threshold: {null_thresh}...')\n",
    "        \n",
    "        nt = int(len(self.df) * null_thresh)\n",
    "        \n",
    "        for col in self.df.columns:\n",
    "            if self.df[col].isnull().sum() > nt and col != self.dependent:\n",
    "                self.df.drop(col, axis=1, inplace=True)\n",
    "                \n",
    "    def fill_na_(self, X):\n",
    "        \"\"\"Will perform a quick processing of NA values.\"\"\"\n",
    "        if self.verbose: print(f'Performing NaN Replacement...')\n",
    "        for col in X.columns:\n",
    "            if X.dtypes[col] == \"object\":\n",
    "                X[col] = X[col].fillna(\"NA\")\n",
    "            else:\n",
    "                X[col] = X[col].fillna(0)\n",
    "        return X\n",
    "    \n",
    "    def set_categorical(self, cat_thresh, X):\n",
    "        \"\"\"\n",
    "        Will create our categorical columns based on our cat algorithm. This is the core of AutoTabularBunch. For messy datasets this may not work well and will require the use of TabularBunch\n",
    "        \"\"\"\n",
    "        assert cat_thresh < 1. and cat_thresh > 0.\n",
    "        cats_mask = ((X.nunique() < int(len(X) * cat_thresh)) & (X.dtypes == \"object\")) | (X.dtypes == 'object')\n",
    "        categorical = [cat for cat, b in cats_mask.items() if b]\n",
    "        \n",
    "        if len(categorical) == 0:\n",
    "            categorical = None # purely continuous variables\n",
    "        return categorical\n",
    "        \n",
    "    \n",
    "    def labelencoding_(self):\n",
    "        if self.verbose: print(f'Performing label encoding operation. This may take a few seconds...')\n",
    "        if self.X_cat is not None:\n",
    "            for col in self.X_cat.columns:\n",
    "                le = LabelEncoder()\n",
    "                self.X_cat[col] = le.fit_transform(self.X_cat[col])\n",
    "                self.X_cat[col] = self.X_cat[col].astype('category')\n",
    "        \n",
    "        # encoding our y if classification\n",
    "        if self.problem_type == 'classification':\n",
    "            le = LabelEncoder()\n",
    "            self.Y = le.fit_transform(self.Y)\n",
    "    \n",
    "    def get_train_test_(self, test_size=0.1):\n",
    "        \"\"\"\n",
    "        This get_train_test_ method will take in a bunch object and append x_train, x_val, y_train, y_val using train_test_split from the scikit learn libray. \n",
    "\n",
    "        This will allow for everything to be kept in a single bunch object which will feed into the Learner class and Dataset class\n",
    "        \"\"\"\n",
    "        self.x_train, self.x_val, self.y_train, self.y_val = train_test_split(self.X,self.Y, test_size=test_size, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# helper function to return categorical variables\n",
    "def get_categorical_(dependent, df):\n",
    "    \"\"\"\n",
    "    Helper function to return categorical variables\n",
    "    TO DO: Need to make this more dynamic, what if there are continuos variables? At the moment this function will turn all but the dependent variable as categorical variables\n",
    "    \"\"\"\n",
    "    return list(df.columns[df.columns != dependent])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Defining our Dependent, Categorical, and Continuous Variables.\n",
    "# In this case, we don't have continuous: so will just feed dependent and cateorical\n",
    "dependent = 'is_female'\n",
    "categorical = get_categorical_(dependent, data_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing NaN Replacement...\n",
      "Performing label encoding operation. This may take a few seconds...\n",
      "Finished!\n"
     ]
    }
   ],
   "source": [
    "# 2. Building tabularbunch\n",
    "# Testing our class & methods\n",
    "# in production: We will first have to determine what is\n",
    "# categorical & continuos. \n",
    "# this will be part of the typical data exploration phase\n",
    "tabularbunch = TabularBunch(df=data_sample, categorical=categorical, dependent=dependent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #export\n",
    "# # 2. \n",
    "# def get_train_test_(bunch, test_size=0.1):\n",
    "#     \"\"\"\n",
    "#     This get_train_test_ function will take in a bunch object and append x_train, x_val, y_train, y_val using train_test_split from the scikit learn libray. \n",
    "    \n",
    "#     This will allow for everything to be kept in a single bunch object which will feed into the Learner class and Dataset class\n",
    "#     \"\"\"\n",
    "#     X, Y = bunch.X, bunch.Y\n",
    "#     bunch.x_train, bunch.x_val, bunch.y_train, bunch.y_val = train_test_split(X,Y, test_size=test_size, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # splitting our data\n",
    "# # this will be appended to the bunch object\n",
    "# get_train_test_(tabularbunch, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting our data\n",
    "# We will simply call our split_test_train_ method \n",
    "# on our tabular bunch object\n",
    "tabularbunch.get_train_test_(test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((450, 1233), (50, 1233))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking sizes to see if they match\n",
    "tabularbunch.x_train.shape, tabularbunch.x_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((450,), (50,))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tabularbunch.y_train.shape, tabularbunch.y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# 2.\n",
    "# tabular dataset class\n",
    "class TabularDataset(Dataset):\n",
    "    def __init__(self, tabularbunch, ds_type='train'):\n",
    "        \"\"\"\n",
    "        This will be our main working dataset class. We will inherit from the Dataset class provided by PyTorch which will allow use to iterate through batches appropriateley when pushed to DataLoaders\n",
    "        \n",
    "        If there are both cat & cont variables, this will simply create a split X1, X2 respectively. \n",
    "        \"\"\"\n",
    "        if ds_type == 'train':\n",
    "            X = tabularbunch.x_train\n",
    "            Y = tabularbunch.y_train\n",
    "        else:\n",
    "            X = tabularbunch.x_val\n",
    "            Y = tabularbunch.y_val\n",
    "\n",
    "        self.X_state = tabularbunch.X_state\n",
    "        \n",
    "        # initial split\n",
    "        # we will split cat & cont variables | IF cont or cat doesn't exist\n",
    "        # this will just create a single X dataset]\n",
    "        if self.X_state == 'both':\n",
    "            self.X1 = X.loc[:,tabularbunch.emb_cols].copy().values.astype(np.int64)\n",
    "            self.X2 = X.drop(columns=tabularbunch.emb_cols).copy().values.astype(np.float32)\n",
    "        elif self.X_state == 'catonly':\n",
    "            self.X1 = X.copy().values.astype(np.int64)\n",
    "        elif self.X_state == 'contonly':\n",
    "            self.X2 = X.copy().values.astype(np.float32)\n",
    "        \n",
    "        if tabularbunch.problem_type == 'classification':\n",
    "            if isinstance(Y, pd.Series):\n",
    "                self.y = Y.values.astype(np.int64)\n",
    "            else:\n",
    "                self.y = Y # already int value\n",
    "        elif tabularbunch.problem_type == 'regression':\n",
    "            if isinstance(Y, pd.Series):\n",
    "                self.y = Y.values.astype(np.float32)\n",
    "            else:\n",
    "                self.y = Y.astype(np.float32) # convert to float32\n",
    "        \n",
    "        # NORMALIZING CONT Dataset | if it exist\n",
    "        if self.X_state == 'contonly' or self.X_state == 'both':\n",
    "            self.X2 = (self.X2 - self.X2.mean()) / self.X2.std()\n",
    "        \n",
    "    def __len__(self): return len(self.y)\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Will iterate through condition: If X2 is not empty we will return both X1 & X2, otherwise just X1\n",
    "        \"\"\"\n",
    "        if self.X_state == 'cat_only': return self.X1[idx], self.y[idx]\n",
    "        elif self.X_state == 'cont_only': return self.X2[idx], self.y[idx]\n",
    "        else: return self.X1[idx], self.X2[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grabbing our train and test datasets\n",
    "# we will feed our tabularbunch object \n",
    "train_ds = TabularDataset(tabularbunch, ds_type='train')\n",
    "valid_ds = TabularDataset(tabularbunch, ds_type='valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to grab train_ds and valid_ds in one grab\n",
    "def get_datasets_(tabularbunch):\n",
    "    \"\"\"\n",
    "    Will return train and valid datasets\n",
    "    \"\"\"\n",
    "    train_ds = TabularDataset(tabularbunch, ds_type='train')\n",
    "    valid_ds = TabularDataset(tabularbunch, ds_type='valid')\n",
    "    return train_ds, valid_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grabbing our train and valid ds using our helper function\n",
    "train_ds, valid_ds = get_datasets_(tabularbunch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have our train and valid datasets we will create\n",
    "# dataloaders using PyTorchs DataLoader class\n",
    "# We will have to feed it bs\n",
    "bs = 5\n",
    "train_dl = DataLoader(train_ds, batch_size=bs, shuffle=True)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3, 19,  0, ...,  1,  0,  1],\n",
       "       [ 1,  9,  5, ...,  1,  2,  1],\n",
       "       [ 0,  2,  0, ...,  1,  3,  1],\n",
       "       ...,\n",
       "       [ 1,  8,  0, ...,  1,  2,  1],\n",
       "       [ 0,  4,  0, ...,  1,  0,  1],\n",
       "       [ 0,  3,  0, ...,  1,  5,  1]], dtype=int64)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how to quickly check the dataset\n",
    "train_dl.dataset.X1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# 3.\n",
    "# A\n",
    "# Creating our DataBunch class which will create data objects that will \n",
    "# feed into our learner class \n",
    "# this will store everything regarding our data\n",
    "# and will make the entire process or creating a tabular dataloader \n",
    "# much easier.\n",
    "# simply put: this puts everything together in one place\n",
    "### V0.1.0 ### \n",
    "class TabularData():\n",
    "    def __init__(self, tabularbunch, test_size:float=0.1, bs:int=64, train_shuffle:bool=True,**kwargs):\n",
    "        \"\"\"\n",
    "        Tabular Data class will host everything associated with tabular data. This will be the main class that our Learner class will use to both train and test on.\n",
    "        \n",
    "        TO DO: \n",
    "        Create save methods. The user | app, should be able to save the dataloader incase of any error. This will allow for them to load the post-processed dataset into a TabularData class which will hold both DataLoaders (train, valid)\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        self.tabularbunch = tabularbunch \n",
    "        self.tabularbunch.get_train_test_(test_size=test_size)\n",
    "        \n",
    "        # grabbing our datasets\n",
    "        train_ds, valid_ds = self.get_datasets_()\n",
    "        \n",
    "        # setting our dataloaders\n",
    "        self.train_dl = DataLoader(train_ds, batch_size=bs, shuffle=train_shuffle, **kwargs)\n",
    "        self.valid_dl = DataLoader(valid_ds, batch_size=bs, **kwargs)\n",
    "        \n",
    "    def get_datasets_(self):\n",
    "        \"\"\"\n",
    "        retrieve test train split\n",
    "        \"\"\"\n",
    "        train_ds = TabularDataset(self.tabularbunch, ds_type='train')\n",
    "        valid_ds = TabularDataset(self.tabularbunch, ds_type='valid')\n",
    "        return train_ds, valid_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting our data with our TabularData function\n",
    "data = TabularData(tabularbunch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Workflow\n",
    "We will now experiment with several datasets and go through the workflow. Lastly, we will feed our ```TabularData``` object into our ```Learner``` object which will do all the training and inference processing. \n",
    "\n",
    "We will cover ```Learner``` in ```02``` notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1. ### \n",
    "# The first thing we will do is grab our raw data and perform some exploration\n",
    "# and any extra pre-processing. This can be from feature extraction\n",
    "# to clairvoyance augmentation\n",
    "df = pd.read_csv('./data/wids/train.csv', low_memory=False).iloc[:100] # grab first 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2. ###\n",
    "# Now we will simply grab variable names if they exist\n",
    "# these will be: Categorical variables, Continuous variables, Dependent variable\n",
    "# nb: Dependent is what we are either predicting or forecasting\n",
    "\n",
    "# this dataset does not contain any continuous variables. We will show this in the\n",
    "# next example\n",
    "dependent = 'is_female'\n",
    "categorical = get_categorical_(dependent, df) # custom functon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing NaN Replacement...\n",
      "Performing label encoding operation. This may take a few seconds...\n",
      "Finished!\n"
     ]
    }
   ],
   "source": [
    "### 3. ###\n",
    "# Creating our tabularbunch\n",
    "# This will be our core object which will perform some initial pre-processing \n",
    "# and will store embs for categorical. This will be fed into our TabularDataset\n",
    "tabularbunch = TabularBunch(df=df, categorical=categorical, dependent=dependent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 4. ###\n",
    "# Creating our Data Object\n",
    "# This will be the main component to proccess the data in our Learner object\n",
    "data = TabularData(tabularbunch, bs=32, num_workers=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Workflow W/ AutoTabularBunch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1. ### \n",
    "# Grabbing our raw data, setting it as a dataframe using pandas DF\n",
    "root = Path('./data')\n",
    "(root/'ibm_attrition').listdir()\n",
    "f = os.listdir(root/'ibm_attrition')[0]\n",
    "data = pd.read_csv(f'{root}/ibm_attrition/{f}', low_memory=False) # this will be a dragon 'drag-on'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2. ### \n",
    "# Creating our TabularBunch\n",
    "# We will be using ```AutoTabularBunch``` which will create categorical, and contincuous variables automatically\n",
    "dependent = 'Attrition'\n",
    "tabularbunch = AutoTabularBunch(df=data, dependent=dependent, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3. ###\n",
    "# Creating our Data Object\n",
    "data = TabularData(tabularbunch, bs=32, num_workers=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Workflow W/ AutoTabularBunch & Cont only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = Path('./data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1. ###\n",
    "# uploading our data\n",
    "f = (root/'eeg').listdir()[0]\n",
    "data = pd.read_csv(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2. ###\n",
    "# dropping column: Our UI will direct this \n",
    "# data.drop(columns='predefinedlabel', inplace=True)\n",
    "dependent = 'user-definedlabeln'\n",
    "\n",
    "# setting our tabularbunch object using Auto\n",
    "tabularbunch = AutoTabularBunch(df=data, dependent=dependent, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3. ###\n",
    "# Creating our data object: The core to our Learner Object\n",
    "data = TabularData(tabularbunch, bs=32, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.28040695, -0.28043163, -0.2802835 , ..., -0.25300834,\n",
       "        -0.2714949 , -0.28043574],\n",
       "       [-0.2804193 , -0.28043163, -0.2802588 , ..., -0.22441238,\n",
       "        -0.22833765, -0.28043574],\n",
       "       [-0.28043163, -0.28043163, -0.2802794 , ..., -0.27439973,\n",
       "        -0.2718734 , -0.28043574],\n",
       "       ...,\n",
       "       [-0.2804193 , -0.28043163, -0.2803658 , ..., -0.23607706,\n",
       "        -0.2234537 , -0.28043574],\n",
       "       [-0.28043574, -0.28041106, -0.2803658 , ..., -0.21134055,\n",
       "        -0.26606783, -0.28043163],\n",
       "       [-0.28041518, -0.28041106, -0.28023824, ..., -0.23924114,\n",
       "        -0.23973076, -0.28043163]], dtype=float32)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.train_dl.dataset.X2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Workflow W/ AutoTabularBunch & Regression Problem Type\n",
    "Not all problems are classification tasks and are ```regression``` in this type of problem we are predicting a continuous variable opposed to a fixed class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1. ###\n",
    "# loading our regression task dataset\n",
    "f = (root/'kepler_exoplanet').listdir()[0]\n",
    "data = pd.read_csv(f)\n",
    "\n",
    "# dropping unessercary col\n",
    "data.drop(columns='rowid', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2. ###\n",
    "# setting our tabularbunch object\n",
    "# our dependent is float\n",
    "# therefor, this will be a 'regression' problem_type which we will define in the class init\n",
    "dependent = 'koi_score'\n",
    "tabularbunch = AutoTabularBunch(df=data, dependent=dependent, problem_type='regression', verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3. ###\n",
    "# Creating our data object\n",
    "data = TabularData(tabularbunch, bs=32, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  nan, 0.   , 0.76 , ..., 0.   ,   nan, 0.001], dtype=float32)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# in production we cannnot use this dataset as our y contains nans\n",
    "data.train_dl.dataset.y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggressive purge\n",
    "This is a forced method for transforming our data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1. ###\n",
    "# loading our regression task dataset\n",
    "f = (root/'kepler_exoplanet').listdir()[0]\n",
    "data = pd.read_csv(f)\n",
    "\n",
    "# dropping unessercary col\n",
    "data.drop(columns='rowid', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2. ###\n",
    "# setting our tabularbunch object\n",
    "# our dependent is float\n",
    "# therefor, this will be a 'regression' problem_type which we will define in the class init\n",
    "dependent = 'koi_score'\n",
    "tabularbunch = AutoTabularBunch(df=data, dependent=dependent, problem_type='regression', purge_='aggressive', verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3. ###\n",
    "# Creating our data object\n",
    "data = TabularData(tabularbunch, bs=32, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7248,)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.train_dl.dataset.y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7248, 5)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.train_dl.dataset.X1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7248, 43)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.train_dl.dataset.X2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mushroom Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = (root/'mushroom').listdir()[0]\n",
    "data = pd.read_csv(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabularbunch = AutoTabularBunch(df=data, dependent='class', verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'catonly'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tabularbunch.X_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = TabularData(tabularbunch, bs=32, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export: V0.1.0 \n",
    "* *Updated 1 12/24/2019* - Christmas Eve! \n",
    "* *Updated 2 12/26/2019* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted copernicusTabular_002.ipynb to exp\\nb_copernicusTabular.py\n"
     ]
    }
   ],
   "source": [
    "!python notebook2script.py copernicusTabular_002.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
